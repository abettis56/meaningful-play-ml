{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaningful Play Score Assigner\n",
    "\n",
    "This program is designed to take an adjacency matrix of a topology of non-looping, non-backtracking linear choices, and apply q-learning to determine how meaningful the set of choices would be from the perspective of the actor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Initial set up before we begin:\n",
    "\n",
    "#### Import Statements\n",
    "\n",
    "Here all libraries that we use will be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import random\n",
    "import xlsxwriter\n",
    "import xml.dom.minidom as minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather User Input\n",
    "\n",
    "We'll need to know the input file for the graph and the number of layers\n",
    "\n",
    "TODO: (Can modify layer and ending counts to be automatically calculated from adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"OnlyIntegrated.xml\" #input(\"Please input the name of the topology .xml file you want to score: \")\n",
    "layers =  5#int(input(\"Please input the number of non-ending layers your topology has: \"))\n",
    "endings = 4 #int(input(\"Please input the number of endings in your topology: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create State Mapping and Adjacency matrix\n",
    "\n",
    "The states need to be put into a map for identification purposes, and an adjacency matrix can also be generated from the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 is: ending_1\n",
      "E2 is: ending_2\n",
      "E3 is: ending_3\n",
      "E4 is: ending_4\n",
      "Matrix:\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "Map:\n",
      "\n",
      "{'Start': 0, '9cuP0UydbNUHG_r8ZG4T-2': 1, '9cuP0UydbNUHG_r8ZG4T-3': 2, '9cuP0UydbNUHG_r8ZG4T-5': 3, 'E1': 4, '9cuP0UydbNUHG_r8ZG4T-9': 5, '9cuP0UydbNUHG_r8ZG4T-10': 6, '9cuP0UydbNUHG_r8ZG4T-12': 7, 'E2': 8, '9cuP0UydbNUHG_r8ZG4T-16': 9, '9cuP0UydbNUHG_r8ZG4T-17': 10, '9cuP0UydbNUHG_r8ZG4T-19': 11, 'E3': 12, '9cuP0UydbNUHG_r8ZG4T-23': 13, '9cuP0UydbNUHG_r8ZG4T-24': 14, '9cuP0UydbNUHG_r8ZG4T-26': 15, 'E4': 16, '9cuP0UydbNUHG_r8ZG4T-30': 17, '9cuP0UydbNUHG_r8ZG4T-32': 18, '9cuP0UydbNUHG_r8ZG4T-34': 19, '9cuP0UydbNUHG_r8ZG4T-36': 20}\n",
      "{'9cuP0UydbNUHG_r8ZG4T-1': 0, '9cuP0UydbNUHG_r8ZG4T-2': 1, '9cuP0UydbNUHG_r8ZG4T-3': 2, '9cuP0UydbNUHG_r8ZG4T-5': 3, '9cuP0UydbNUHG_r8ZG4T-6': 4, '9cuP0UydbNUHG_r8ZG4T-9': 5, '9cuP0UydbNUHG_r8ZG4T-10': 6, '9cuP0UydbNUHG_r8ZG4T-12': 7, '9cuP0UydbNUHG_r8ZG4T-13': 8, '9cuP0UydbNUHG_r8ZG4T-16': 9, '9cuP0UydbNUHG_r8ZG4T-17': 10, '9cuP0UydbNUHG_r8ZG4T-19': 11, '9cuP0UydbNUHG_r8ZG4T-20': 12, '9cuP0UydbNUHG_r8ZG4T-23': 13, '9cuP0UydbNUHG_r8ZG4T-24': 14, '9cuP0UydbNUHG_r8ZG4T-26': 15, '9cuP0UydbNUHG_r8ZG4T-27': 16, '9cuP0UydbNUHG_r8ZG4T-30': 17, '9cuP0UydbNUHG_r8ZG4T-32': 18, '9cuP0UydbNUHG_r8ZG4T-34': 19, '9cuP0UydbNUHG_r8ZG4T-36': 20}\n"
     ]
    }
   ],
   "source": [
    "#info: https://www.guru99.com/manipulating-xml-with-python.html#3\n",
    "\n",
    "#Mapping for the states\n",
    "location_to_state = {}\n",
    "\n",
    "#Map to help finish populating (imperfect copy of location_to_state)\n",
    "tempMap = {}\n",
    "\n",
    "doc = minidom.parse(filename)\n",
    "\n",
    "#load in all the relevant elements to a list called \"cells\"\n",
    "cells = doc.getElementsByTagName(\"mxCell\")\n",
    "\n",
    "#list to hold nodes\n",
    "nodes = []\n",
    "#list to hold connections between nodes\n",
    "connections = []\n",
    "#Number to keep track of how many endings have been added\n",
    "endingIndex = 1\n",
    "\n",
    "#loop through cells to sort ellipses and endArrows into their respective lists\n",
    "for label in cells:\n",
    "    if(label.getAttribute(\"style\") != \"\"):\n",
    "        if(label.getAttribute(\"style\").startswith(\"ellipse\")):\n",
    "            nodes.append(label)\n",
    "        elif(label.getAttribute(\"style\").startswith(\"endArrow\")):\n",
    "            connections.append(label)\n",
    "\n",
    "#declare empty adjacency matrix to populate with data on the graph\n",
    "rewards = []\n",
    "\n",
    "#populate matrix and map simulatneously as you iterate through the nodes\n",
    "for i in range(len(nodes)):\n",
    "    rewards.append([])\n",
    "    if \"ending\" in nodes[i].getAttribute(\"value\"):\n",
    "        location_to_state[\"E\" + str(endingIndex)] = i\n",
    "        print(\"E\" + str(endingIndex) + \" is: \" + nodes[i].getAttribute(\"value\"))\n",
    "        endingIndex = endingIndex + 1\n",
    "    elif \"start\" in nodes[i].getAttribute(\"value\"):\n",
    "        location_to_state[\"Start\"] = i\n",
    "    else:\n",
    "        location_to_state[nodes[i].getAttribute(\"id\")] = i\n",
    "    \n",
    "    tempMap[nodes[i].getAttribute(\"id\")] = i\n",
    "    \n",
    "    for j in range(len(nodes)):\n",
    "        rewards[i].append(0)\n",
    "\n",
    "#finish populating the adjacency matrix with data on the connections between nodes\n",
    "for link in connections:\n",
    "    x = tempMap[link.getAttribute(\"source\")]\n",
    "    y = tempMap[link.getAttribute(\"target\")]\n",
    "    rewards[x][y] = 1\n",
    "\n",
    "#convert matrix into nparray and print it and the map for manual inspection\n",
    "rewards = np.asarray(rewards)\n",
    "print(\"Matrix:\\n\")\n",
    "print(rewards)\n",
    "print(\"\\nMap:\\n\")\n",
    "print(location_to_state)\n",
    "\n",
    "print(tempMap)\n",
    "\n",
    "# Map indices to locations\n",
    "state_to_location = dict((state,location) for location,state in location_to_state.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define q-learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    def __init__(self, alpha, gamma, epsilon, location_to_state, rewards, state_to_location, Q):\n",
    "        \"\"\" Initialize alpha, gamma, epsilon, states, actions, rewards, and Q-values\n",
    "        \"\"\"\n",
    "        self.gamma = gamma  \n",
    "        self.alpha = alpha \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        self.Q = Q\n",
    "        \n",
    "    def training(self, start_location, end_location, iterations):\n",
    "        \"\"\"Training the system in the given environment to move from a start state to an end state\n",
    "        \"\"\"\n",
    "        rewards_new = np.copy(self.rewards)\n",
    "        \n",
    "        #set reward for end state to 100 to incentivize reaching desired end\n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        rewards_new[ending_state, ending_state] = 100\n",
    "\n",
    "        #Loop for specified number of iterations\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            #initialize current state as the start state\n",
    "            #current_state = self.location_to_state[start_location]\n",
    "            \n",
    "            #Randomly pick a state to observe\n",
    "            current_state = np.random.randint(0,len(self.rewards)) \n",
    "            \n",
    "            #counter to make sure it hits a dead end after a while\n",
    "            counter = 0\n",
    "            \n",
    "            #infinite loop runs through route, updating until it hits a dead-end\n",
    "            while(True):\n",
    "                \n",
    "                #Iterate counter. If it hits the limit, break the loop and print error message\n",
    "                counter += 1\n",
    "                if(counter == limit):\n",
    "                    print(\"Error: Hit limit before reaching an end while training\")\n",
    "                    break\n",
    "                \n",
    "                #Construct list of possible actions\n",
    "                playable_actions = []\n",
    "                \n",
    "                for j in range(len(self.rewards)):\n",
    "                    if rewards_new[current_state,j] > 0:\n",
    "                        playable_actions.append(j)\n",
    "\n",
    "                #Only run updates if observed state has performable actions\n",
    "                if playable_actions:\n",
    "                    #Decide whether to random walk or follow standard policy\n",
    "                    if random.uniform(0, 1) < epsilon:\n",
    "                        next_state = np.random.choice(playable_actions)\n",
    "                    else:\n",
    "                        next_state = np.argmax(rewards_new[current_state,])\n",
    "\n",
    "                    #Calculate temporal difference\n",
    "                    TD = rewards_new[current_state,next_state] + \\\n",
    "                            self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state,next_state]\n",
    "\n",
    "                    #updates Q-value using Bellman equation\n",
    "                    self.Q[current_state,next_state] += self.alpha * TD\n",
    "                    \n",
    "                    #check if agent is at desired ending\n",
    "                    if next_state == current_state:\n",
    "                        break\n",
    "                    \n",
    "                    #update current state to move forward\n",
    "                    current_state = next_state\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "        \n",
    "        # Get the route \n",
    "        return self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "        \n",
    "    # Get the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        \n",
    "        #set counter to break if it learns wrong route\n",
    "        counter = 0\n",
    "        \n",
    "        while(next_location != end_location):\n",
    "            #Iterate counter. If it hits the limit, break the loop and print error message\n",
    "            counter += 1\n",
    "            if(counter >= limit):\n",
    "                print(\"Error: Failed to learn correct route\")\n",
    "                return None\n",
    "            \n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location            \n",
    "\n",
    "        return route\n",
    "    \n",
    "#Take set of q-tables and average them into one q-table\n",
    "def qaverage(table_set):\n",
    "    num = 0\n",
    "    output_table = table_set[0].copy()\n",
    "    for i in range(len(table_set[0][0])):\n",
    "        for j in range(len(table_set[0])):\n",
    "            for k in range(len(table_set)):\n",
    "                num += table_set[k][j][i]\n",
    "            output_table[j][i] = num / len(table_set)\n",
    "            num = 0\n",
    "\n",
    "    return output_table\n",
    "    \n",
    "# Initialize parameters\n",
    "gamma = 0.75 # Discount factor (discounts previous rewards)\n",
    "alpha = 0.9 # Learning rate\n",
    "epsilon = 0.2 # Exploration vs exploitation percentage\n",
    "\n",
    "limit = 100000 # number of steps until a dead end is hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q-learning\n",
    "\n",
    "So now that all the setup has been done, we can start our q-learning algorithm, then move on to processing its output.\n",
    "\n",
    "#### Define q-learning Execution Functions\n",
    "\n",
    "We need a couple of functions to handle our q-learning, since we need to execute multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle all q-learning for a given topology\n",
    "def qmaster(final_state, output_tables):\n",
    "    #array to store the final Q-Table of each 1000 iterations\n",
    "    qtables = []\n",
    "    for i in range(10):\n",
    "        qagent = QAgent(alpha, gamma, epsilon, location_to_state, rewards,  state_to_location, \n",
    "                        np.array(np.zeros([len(location_to_state),len(location_to_state)])))\n",
    "        #TODO: REMOVE\n",
    "        print(\"Starting training with ending: \", final_state)\n",
    "    \n",
    "        training_results = qagent.training(\"Start\", final_state, 100000)\n",
    "        \n",
    "        #TODO: REMOVE\n",
    "        print(\"Done with training\")\n",
    "        if training_results is not None:\n",
    "            qtables.append(qagent.Q)\n",
    "\n",
    "    if len(qtables) > 0:\n",
    "        output_tables.append(qaverage(qtables))\n",
    "        \n",
    "#generates excel spreadsheet containing all q-tables in a given path\n",
    "def to_excel(qtables, excel_name):\n",
    "    \"\"\"store data in excel\n",
    "    \"\"\"\n",
    "    workbook = xlsxwriter.Workbook(excel_name + \".xlsx\")\n",
    "\n",
    "    #write each q-table to another worksheet\n",
    "    for i in range(len(qtables)):\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        for j in range(len(qtables[i])):\n",
    "            for k in range(len(qtables[i])):\n",
    "                worksheet.write(j, k, qtables[i][j][k])\n",
    "\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run q-learning algorithm\n",
    "\n",
    "Finally, we can run q-master and store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E1\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E2\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E3\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n",
      "Starting training with ending:  E4\n",
      "Done with training\n"
     ]
    }
   ],
   "source": [
    "#an array to hold the outputs of q-master.\n",
    "averaged_tables = []\n",
    "\n",
    "#run qmaster for each ending state\n",
    "for i in range(endings):\n",
    "    qmaster(\"E\" + str(i + 1), averaged_tables)\n",
    "    \n",
    "to_excel(averaged_tables, \"Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weights\n",
    "\n",
    "Calculate and apply weights to each of the averaged q-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12, 0.15999999999999998, 0.19999999999999998, 0.24, 0.28]\n",
      "Going to:0,17\n",
      "Going to:17,1\n",
      "Going to:1,2\n",
      "Going to:2,3\n",
      "Going to:3,4\n",
      "Going to:0,18\n",
      "Going to:18,5\n",
      "Going to:5,6\n",
      "Going to:6,7\n",
      "Going to:7,8\n",
      "Going to:0,19\n",
      "Going to:19,9\n",
      "Going to:9,10\n",
      "Going to:10,11\n",
      "Going to:11,12\n",
      "Going to:0,20\n",
      "Going to:20,13\n",
      "Going to:13,14\n",
      "Going to:14,15\n",
      "Going to:15,16\n",
      "[0.12, 0.15999999999999998, 0.19999999999999998, 0.24, 0.28]\n",
      "Going to:0,17\n",
      "Going to:17,1\n",
      "Going to:1,2\n",
      "Going to:2,3\n",
      "Going to:3,4\n",
      "Going to:0,18\n",
      "Going to:18,5\n",
      "Going to:5,6\n",
      "Going to:6,7\n",
      "Going to:7,8\n",
      "Going to:0,19\n",
      "Going to:19,9\n",
      "Going to:9,10\n",
      "Going to:10,11\n",
      "Going to:11,12\n",
      "Going to:0,20\n",
      "Going to:20,13\n",
      "Going to:13,14\n",
      "Going to:14,15\n",
      "Going to:15,16\n",
      "[0.12, 0.15999999999999998, 0.19999999999999998, 0.24, 0.28]\n",
      "Going to:0,17\n",
      "Going to:17,1\n",
      "Going to:1,2\n",
      "Going to:2,3\n",
      "Going to:3,4\n",
      "Going to:0,18\n",
      "Going to:18,5\n",
      "Going to:5,6\n",
      "Going to:6,7\n",
      "Going to:7,8\n",
      "Going to:0,19\n",
      "Going to:19,9\n",
      "Going to:9,10\n",
      "Going to:10,11\n",
      "Going to:11,12\n",
      "Going to:0,20\n",
      "Going to:20,13\n",
      "Going to:13,14\n",
      "Going to:14,15\n",
      "Going to:15,16\n",
      "[0.12, 0.15999999999999998, 0.19999999999999998, 0.24, 0.28]\n",
      "Going to:0,17\n",
      "Going to:17,1\n",
      "Going to:1,2\n",
      "Going to:2,3\n",
      "Going to:3,4\n",
      "Going to:0,18\n",
      "Going to:18,5\n",
      "Going to:5,6\n",
      "Going to:6,7\n",
      "Going to:7,8\n",
      "Going to:0,19\n",
      "Going to:19,9\n",
      "Going to:9,10\n",
      "Going to:10,11\n",
      "Going to:11,12\n",
      "Going to:0,20\n",
      "Going to:20,13\n",
      "Going to:13,14\n",
      "Going to:14,15\n",
      "Going to:15,16\n"
     ]
    }
   ],
   "source": [
    "#Calculate weights\n",
    "def weight_calculator(layers):\n",
    "    #get slope\n",
    "    slope = 1 / (layers * layers)\n",
    "    #get sum\n",
    "    sum = 0\n",
    "    for i in range(1, layers + 1):\n",
    "        sum += (i * slope)\n",
    "    \n",
    "    #get amount to add to equal 1\n",
    "    toAdd = (1 - sum) / layers\n",
    "    \n",
    "    #Finally, set up and return array of weights\n",
    "    weights = []\n",
    "    for i in range(1, layers + 1):\n",
    "        weights.append((i * slope) + toAdd)\n",
    "        \n",
    "    print(weights)\n",
    "    return weights\n",
    "\n",
    "#list of visited nodes for weighting function to ignore\n",
    "visited = []\n",
    "\n",
    "#Apply weighting function to give high score to early states\n",
    "def apply_weights_helper(array, layers, endings):\n",
    "    global visited\n",
    "    #zero out the diagonal of the array to eliminate reward values, and eliminate any links to the start state\n",
    "    for i in range(len(array)):\n",
    "        array[i][i] = 0\n",
    "        #array[i][0] = 0\n",
    "    \n",
    "    weights = weight_calculator(int(layers))\n",
    "    weights.reverse()\n",
    "    apply_weights(array, weights, location_to_state['Start'], 0)\n",
    "    visited = []\n",
    "    \n",
    "def apply_weights(array, weights, x, level):\n",
    "    for i in range(len(array[x])):\n",
    "        if(level < len(weights)):\n",
    "            array[x][i] = array[x][i] * weights[level]\n",
    "        if(array[x][i] > 0 and i not in visited):\n",
    "            print(\"Going to:\" + str(x) + \",\" + str(i))\n",
    "            visited.append(i)\n",
    "            apply_weights(array, weights, i, (level + 1))\n",
    "       \n",
    "for i in range(len(averaged_tables)):\n",
    "    apply_weights_helper(averaged_tables[i], layers, endings)\n",
    "    \n",
    "to_excel(averaged_tables, \"Weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing\n",
    "\n",
    "Before we take the pairwise minkowski distance to get our score, we want to normalize our weighted q-tables.\n",
    "We can do this by running each subarray of each table through the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE--------------------------------------\n",
      "[27.432343749999994, 0.85421875, 0.85421875, 0.85421875, 34.21249999999999, 36.27999999999999, 36.11999999999999, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 31.031249999999993, 0.65625, 0.65625, 0.65625]\n",
      "AFTER---------------------------------------\n",
      "[7.243702111673905e-05, 2.076000985538585e-16, 2.076000985538585e-16, 2.076000985538585e-16, 0.06375951839885427, 0.5040212758207727, 0.4294985996974986, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 0.0026481690617552755, 1.7031418459713627e-16, 1.7031418459713627e-16, 1.7031418459713627e-16]\n",
      "BEFORE--------------------------------------\n",
      "[0.85421875, 27.432343749999994, 0.85421875, 0.85421875, 0.46249999999999997, 0.27999999999999997, 0.12, 34.21249999999999, 36.27999999999999, 36.11999999999999, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 0.65625, 31.031249999999993, 0.65625, 0.65625]\n",
      "AFTER---------------------------------------\n",
      "[2.076000985538585e-16, 7.243702111673905e-05, 2.076000985538585e-16, 2.076000985538585e-16, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 0.06375951839885427, 0.5040212758207727, 0.4294985996974986, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 1.4031569889271156e-16, 1.16908885619479e-16, 9.962318075559989e-17, 1.7031418459713627e-16, 0.0026481690617552755, 1.7031418459713627e-16, 1.7031418459713627e-16]\n",
      "BEFORE--------------------------------------\n",
      "[0.85421875, 0.85421875, 27.432343749999994, 0.85421875, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 34.21249999999999, 36.27999999999999, 36.11999999999999, 0.46249999999999997, 0.27999999999999997, 0.12, 0.65625, 0.65625, 31.031249999999993, 0.65625]\n",
      "AFTER---------------------------------------\n",
      "[2.0760009855385848e-16, 2.0760009855385848e-16, 7.243702111673904e-05, 2.0760009855385848e-16, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 0.06375951839885426, 0.5040212758207726, 0.42949859969749854, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 1.7031418459713624e-16, 1.7031418459713624e-16, 0.0026481690617552755, 1.7031418459713624e-16]\n",
      "BEFORE--------------------------------------\n",
      "[0.85421875, 0.85421875, 0.85421875, 27.432343749999994, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 0.46249999999999997, 0.27999999999999997, 0.12, 34.21249999999999, 36.27999999999999, 36.11999999999999, 0.65625, 0.65625, 0.65625, 31.031249999999993]\n",
      "AFTER---------------------------------------\n",
      "[2.0760009855385848e-16, 2.0760009855385848e-16, 2.0760009855385848e-16, 7.243702111673904e-05, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 1.4031569889271154e-16, 1.1690888561947897e-16, 9.962318075559988e-17, 0.06375951839885426, 0.5040212758207726, 0.42949859969749854, 1.7031418459713624e-16, 1.7031418459713624e-16, 1.7031418459713624e-16, 0.0026481690617552755]\n"
     ]
    }
   ],
   "source": [
    "#Softmax implementation modified from\n",
    "#https://intellipaat.com/community/942/how-to-implement-the-softmax-function-in-python\n",
    "\n",
    "def softmax(x): \n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\" \n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return (e_x / e_x.sum(axis=0)).tolist()\n",
    "\n",
    "def normalize(array, endings):\n",
    "    processedList = []\n",
    "    indices = []\n",
    "    pair = []\n",
    "    for i in range(len(array)):\n",
    "        for j in range(len(array)):\n",
    "            if(array[i][j] > 0):\n",
    "                pair.append(i)\n",
    "                pair.append(j)\n",
    "                indices.append(pair.copy())\n",
    "                pair = []\n",
    "                processedList.append(array[i][j])\n",
    "        \n",
    "    print(\"BEFORE--------------------------------------\")\n",
    "    print(processedList)\n",
    "    processedList = softmax(processedList)\n",
    "    print(\"AFTER---------------------------------------\")\n",
    "    print(processedList)\n",
    "    for j in range(len(indices)):\n",
    "        array[indices[j][0]][indices[j][1]] = processedList[j]\n",
    "            \n",
    "for i in range(len(averaged_tables)):\n",
    "    normalize(averaged_tables[i], endings)\n",
    "    \n",
    "to_excel(averaged_tables, \"Normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Minkowski Distances and Output the Score\n",
    "\n",
    "We're finally ready to calculate the minkowski distance and output a meaningfulness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000\n"
     ]
    }
   ],
   "source": [
    "#Minkowski Distance implementation altered from\n",
    "#https://www.geeksforgeeks.org/minkowski-distance-python/\n",
    "\n",
    "#convert array into 1D vector for ease of manipulation\n",
    "def vectorize(input_array):\n",
    "    output_array = []\n",
    "    for i in range(len(input_array)):\n",
    "        for j in range(len(input_array[0])):\n",
    "            output_array.append(input_array[i][j])\n",
    "            \n",
    "    return output_array\n",
    "\n",
    "#Calculate Minkowski distance between arrays\n",
    "  \n",
    "# Function distance between two points  \n",
    "# and calculate distance value to given \n",
    "# root value(p is root value) \n",
    "def p_root(value, root): \n",
    "      \n",
    "    root_value = 1 / float(root) \n",
    "    return round (Decimal(value) **\n",
    "             Decimal(root_value), 3) \n",
    "  \n",
    "def minkowski_distance(x, y, p_value): \n",
    "    # pass the p_root function to calculate \n",
    "    # all the values of vector in parallel \n",
    "    return (p_root(sum(pow(abs(a-b), p_value) \n",
    "            for a, b in zip(x, y)), p_value))\n",
    "\n",
    "#vectorize averaged tables\n",
    "vectors = []\n",
    "for i in range(len(averaged_tables)):\n",
    "    vectors.append(vectorize(averaged_tables[i]))\n",
    "    \n",
    "#calculate minkowski distances in a pairwise fashion\n",
    "distances = []\n",
    "acc = 0\n",
    "for i in range(len(averaged_tables) - 1):\n",
    "    for j in range(i + 1, len(vectors)):\n",
    "        #Note: Distance must be divided by 2, because when two nodes differ in a topology, \n",
    "        #the q-values are different in two different places by the minkowski distance function\n",
    "        distance = minkowski_distance(vectors[i], vectors[j], 1) / 2\n",
    "        distances.append(distance)\n",
    "        acc += distance\n",
    "        \n",
    "print((acc / len(distances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
